{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmehrabzaidi/AI-LAB/blob/main/fine_tune_medical_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTrEcT1QF4mv"
      },
      "source": [
        "📦 Cell 1: Install Required Libraries\n",
        "Start by installing necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8IF3J6DVEJyt",
        "outputId": "fd4eb5b7-2a86-43ab-8a7c-f55d4e893b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTD144GnGGW6"
      },
      "source": [
        "##**Create a Tiny Sample Dataset**\n",
        "\n",
        "prepeared data set of 300 to 400 records for entity extraction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7RAWpoexg-uo",
        "outputId": "689488c1-b22b-4cc0-875f-f91f0eed68f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'prompt': \"I'm suffering from acidity, any recommended drugs?\", 'response': '{\"entities\": [{\"text\": \"Pantoprazole\", \"type\": \"Medication\"}, {\"text\": \"Ranitidine\", \"type\": \"Medication\"}]}'}, {'prompt': 'Can you suggest tablets for body pain?', 'response': '{\"entities\": [{\"text\": \"Paracetamol\", \"type\": \"Medication\"}, {\"text\": \"Naproxen\", \"type\": \"Medication\"}, {\"text\": \"Diclofenac\", \"type\": \"Medication\"}]}'}, {'prompt': \"I'm suffering from fever, any recommended drugs?\", 'response': '{\"entities\": [{\"text\": \"Ibuprofen\", \"type\": \"Medication\"}, {\"text\": \"Paracetamol\", \"type\": \"Medication\"}]}'}, {'prompt': 'I have been having body pain. What medicine should I take?', 'response': '{\"entities\": [{\"text\": \"Naproxen\", \"type\": \"Medication\"}]}'}, {'prompt': \"I'm suffering from headache, any recommended drugs?\", 'response': '{\"entities\": [{\"text\": \"Paracetamol\", \"type\": \"Medication\"}]}'}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'response'],\n",
              "    num_rows: 420\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#meducal dataset\n",
        "\n",
        "\n",
        "import random\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Symptoms and related medications\n",
        "symptoms_to_medications = {\n",
        "    \"fever\": [\"Paracetamol\", \"Ibuprofen\", \"Aspirin\"],\n",
        "    \"headache\": [\"Paracetamol\", \"Ibuprofen\", \"Sumatriptan\"],\n",
        "    \"cold\": [\"Cetirizine\", \"Loratadine\", \"Levocetirizine\"],\n",
        "    \"cough\": [\"Dextromethorphan\", \"Codeine\", \"Guaifenesin\"],\n",
        "    \"diarrhea\": [\"Loperamide\", \"ORS\", \"Racecadotril\"],\n",
        "    \"vomiting\": [\"Ondansetron\", \"Domperidone\", \"Metoclopramide\"],\n",
        "    \"motion sickness\": [\"Dimenhydrinate\", \"Meclizine\", \"Promethazine\"],\n",
        "    \"body pain\": [\"Paracetamol\", \"Naproxen\", \"Diclofenac\"],\n",
        "    \"allergy\": [\"Cetirizine\", \"Chlorpheniramine\", \"Fexofenadine\"],\n",
        "    \"acidity\": [\"Pantoprazole\", \"Omeprazole\", \"Ranitidine\"]\n",
        "}\n",
        "\n",
        "# Natural language templates\n",
        "templates = [\n",
        "    \"I have been having {symptom}. What medicine should I take?\",\n",
        "    \"Can you suggest tablets for {symptom}?\",\n",
        "    \"What are the best medications for treating {symptom}?\",\n",
        "    \"I'm suffering from {symptom}, any recommended drugs?\",\n",
        "    \"What tablets help with {symptom}?\",\n",
        "    \"Please suggest some medicine for {symptom}.\",\n",
        "    \"I'm experiencing {symptom}. What can I take?\",\n",
        "]\n",
        "\n",
        "def generate_medical_sample():\n",
        "    symptom = random.choice(list(symptoms_to_medications.keys()))\n",
        "    meds = random.sample(symptoms_to_medications[symptom], k=random.randint(1, len(symptoms_to_medications[symptom])))\n",
        "    prompt = random.choice(templates).format(symptom=symptom)\n",
        "    response_text = f\"For {symptom}, you can take \" + \", \".join(meds[:-1]) + (\" and \" if len(meds) > 1 else \"\") + meds[-1] + \".\"\n",
        "\n",
        "    # Build entity list\n",
        "    entities = [{\"text\": med, \"type\": \"Medication\"} for med in meds]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": json.dumps({\"entities\": entities})\n",
        "    }\n",
        "\n",
        "# Generate 420 examples\n",
        "synthetic_data = [generate_medical_sample() for _ in range(420)]\n",
        "print(synthetic_data[:5])  # View sample\n",
        "\n",
        "# Convert to HuggingFace dataset\n",
        "dataset = Dataset.from_list(synthetic_data)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8qKcO9E0bt49",
        "outputId": "a5770977-2f14-4ec8-8770-f03704cde05d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport random\\nimport json\\nfrom datasets import Dataset\\n\\n# Define entity pools\\nnames = [\"Alice Johnson\", \"Bob Smith\", \"Charlie Davis\", \"Diana Green\", \"Ethan Brown\", \"Fiona White\", \"George Young\", \"Hannah Black\", \"Isaac Moore\", \"Julia Scott\"]\\ncompanies = [\"Google\", \"OpenAI\", \"Microsoft\", \"Apple\", \"Meta\", \"Amazon\", \"Netflix\", \"Tesla\", \"Nvidia\", \"Adobe\"]\\nprofessions = [\"engineer\", \"designer\", \"manager\", \"researcher\", \"scientist\", \"consultant\", \"analyst\", \"developer\", \"specialist\", \"director\"]\\nlocations = [\"New York\", \"San Francisco\", \"London\", \"Berlin\", \"Toronto\", \"Tokyo\", \"Paris\", \"Sydney\", \"Singapore\", \"Dubai\"]\\nweather = [\"sunny\", \"rainy\", \"snowy\", \"cloudy\", \"stormy\", \"foggy\", \"windy\", \"humid\"]\\ncars = [\"Tesla Model 3\", \"Ford Mustang\", \"Chevrolet Camaro\", \"BMW 3 Series\", \"Audi A4\", \"Mercedes Benz C-Class\", \"Honda Civic\", \"Toyota Corolla\", \"Porsche 911\", \"Nissan Altima\"]\\nevents = [\"conference\", \"festival\", \"webinar\", \"convention\", \"hackathon\", \"workshop\", \"trade show\", \"meeting\"]\\nproducts = [\"iPhone 14\", \"Galaxy S23\", \"MacBook Pro\", \"Surface Laptop\", \"PlayStation 5\", \"Nintendo Switch\", \"iPad\", \"Kindle Paperwhite\", \"GoPro Hero 10\"]\\n\\n# Define templates for more variability\\ntemplates = {\\n    \"person\": [\\n        \"{name} works at {company} as a {profession}.\",\\n        \"As a {profession}, {name} joined {company} last year.\",\\n        \"{company} hired {name} for their expertise as a {profession}.\"\\n    ],\\n    \"company\": [\\n        \"{company} launched a new product today.\",\\n        \"{company} announced a major acquisition this morning.\",\\n        \"{company} is expanding its operations globally.\"\\n    ],\\n    \"weather\": [\\n        \"The weather today is {weather_condition} in {location}.\",\\n        \"{location} is experiencing a {weather_condition} day.\",\\n        \"It\\'s been quite {weather_condition} recently in {location}.\"\\n    ],\\n    \"car\": [\\n        \"The new {car_model} is available in {location}.\",\\n        \"I saw a {car_model} driving through {location} yesterday.\",\\n        \"Auto dealers in {location} now offer the {car_model}.\"\\n    ],\\n    \"event\": [\\n        \"The {event} will take place in {location} next week.\",\\n        \"{location} will host the annual {event} this month.\",\\n        \"A global {event} is scheduled soon in {location}.\"\\n    ],\\n    \"product\": [\\n        \"The new {product} was released by {company}.\",\\n        \"{company} unveiled the {product} in a live event.\",\\n        \"Tech fans are excited about the {product} launched by {company}.\"\\n    ]\\n}\\n\\ndef generate_sample():\\n    name = random.choice(names)\\n    company = random.choice(companies)\\n    profession = random.choice(professions)\\n    location = random.choice(locations)\\n    weather_condition = random.choice(weather)\\n    car_model = random.choice(cars)\\n    event = random.choice(events)\\n    product = random.choice(products)\\n\\n    entity_type = random.choice(list(templates.keys()))\\n    template = random.choice(templates[entity_type])\\n\\n    sentence = template.format(\\n        name=name,\\n        company=company,\\n        profession=profession,\\n        location=location,\\n        weather_condition=weather_condition,\\n        car_model=car_model,\\n        event=event,\\n        product=product\\n    )\\n\\n    if entity_type == \"person\":\\n        entities = [{\"text\": name, \"type\": \"Person\"}, {\"text\": company, \"type\": \"Organization\"}]\\n    elif entity_type == \"company\":\\n        entities = [{\"text\": company, \"type\": \"Organization\"}]\\n    elif entity_type == \"weather\":\\n        entities = [{\"text\": weather_condition, \"type\": \"Weather\"}, {\"text\": location, \"type\": \"Location\"}]\\n    elif entity_type == \"car\":\\n        entities = [{\"text\": car_model, \"type\": \"Car\"}, {\"text\": location, \"type\": \"Location\"}]\\n    elif entity_type == \"event\":\\n        entities = [{\"text\": event, \"type\": \"Event\"}, {\"text\": location, \"type\": \"Location\"}]\\n    elif entity_type == \"product\":\\n        entities = [{\"text\": product, \"type\": \"Product\"}, {\"text\": company, \"type\": \"Organization\"}]\\n\\n    return {\\n        \"prompt\": f\"Extract entities in JSON format from: {sentence}\",\\n        \"response\": json.dumps({\"entities\": entities})\\n    }\\n\\n# Generate dataset\\nsynthetic_data = [generate_sample() for _ in range(1200)]  # Increase for more variety\\n\\n# Preview sample\\nprint(synthetic_data[:3])\\n\\n# Convert to HuggingFace Dataset\\ndataset = Dataset.from_list(synthetic_data)\\nprint(dataset)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "import random\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define entity pools\n",
        "names = [\"Alice Johnson\", \"Bob Smith\", \"Charlie Davis\", \"Diana Green\", \"Ethan Brown\", \"Fiona White\", \"George Young\", \"Hannah Black\", \"Isaac Moore\", \"Julia Scott\"]\n",
        "companies = [\"Google\", \"OpenAI\", \"Microsoft\", \"Apple\", \"Meta\", \"Amazon\", \"Netflix\", \"Tesla\", \"Nvidia\", \"Adobe\"]\n",
        "professions = [\"engineer\", \"designer\", \"manager\", \"researcher\", \"scientist\", \"consultant\", \"analyst\", \"developer\", \"specialist\", \"director\"]\n",
        "locations = [\"New York\", \"San Francisco\", \"London\", \"Berlin\", \"Toronto\", \"Tokyo\", \"Paris\", \"Sydney\", \"Singapore\", \"Dubai\"]\n",
        "weather = [\"sunny\", \"rainy\", \"snowy\", \"cloudy\", \"stormy\", \"foggy\", \"windy\", \"humid\"]\n",
        "cars = [\"Tesla Model 3\", \"Ford Mustang\", \"Chevrolet Camaro\", \"BMW 3 Series\", \"Audi A4\", \"Mercedes Benz C-Class\", \"Honda Civic\", \"Toyota Corolla\", \"Porsche 911\", \"Nissan Altima\"]\n",
        "events = [\"conference\", \"festival\", \"webinar\", \"convention\", \"hackathon\", \"workshop\", \"trade show\", \"meeting\"]\n",
        "products = [\"iPhone 14\", \"Galaxy S23\", \"MacBook Pro\", \"Surface Laptop\", \"PlayStation 5\", \"Nintendo Switch\", \"iPad\", \"Kindle Paperwhite\", \"GoPro Hero 10\"]\n",
        "\n",
        "# Define templates for more variability\n",
        "templates = {\n",
        "    \"person\": [\n",
        "        \"{name} works at {company} as a {profession}.\",\n",
        "        \"As a {profession}, {name} joined {company} last year.\",\n",
        "        \"{company} hired {name} for their expertise as a {profession}.\"\n",
        "    ],\n",
        "    \"company\": [\n",
        "        \"{company} launched a new product today.\",\n",
        "        \"{company} announced a major acquisition this morning.\",\n",
        "        \"{company} is expanding its operations globally.\"\n",
        "    ],\n",
        "    \"weather\": [\n",
        "        \"The weather today is {weather_condition} in {location}.\",\n",
        "        \"{location} is experiencing a {weather_condition} day.\",\n",
        "        \"It's been quite {weather_condition} recently in {location}.\"\n",
        "    ],\n",
        "    \"car\": [\n",
        "        \"The new {car_model} is available in {location}.\",\n",
        "        \"I saw a {car_model} driving through {location} yesterday.\",\n",
        "        \"Auto dealers in {location} now offer the {car_model}.\"\n",
        "    ],\n",
        "    \"event\": [\n",
        "        \"The {event} will take place in {location} next week.\",\n",
        "        \"{location} will host the annual {event} this month.\",\n",
        "        \"A global {event} is scheduled soon in {location}.\"\n",
        "    ],\n",
        "    \"product\": [\n",
        "        \"The new {product} was released by {company}.\",\n",
        "        \"{company} unveiled the {product} in a live event.\",\n",
        "        \"Tech fans are excited about the {product} launched by {company}.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def generate_sample():\n",
        "    name = random.choice(names)\n",
        "    company = random.choice(companies)\n",
        "    profession = random.choice(professions)\n",
        "    location = random.choice(locations)\n",
        "    weather_condition = random.choice(weather)\n",
        "    car_model = random.choice(cars)\n",
        "    event = random.choice(events)\n",
        "    product = random.choice(products)\n",
        "\n",
        "    entity_type = random.choice(list(templates.keys()))\n",
        "    template = random.choice(templates[entity_type])\n",
        "\n",
        "    sentence = template.format(\n",
        "        name=name,\n",
        "        company=company,\n",
        "        profession=profession,\n",
        "        location=location,\n",
        "        weather_condition=weather_condition,\n",
        "        car_model=car_model,\n",
        "        event=event,\n",
        "        product=product\n",
        "    )\n",
        "\n",
        "    if entity_type == \"person\":\n",
        "        entities = [{\"text\": name, \"type\": \"Person\"}, {\"text\": company, \"type\": \"Organization\"}]\n",
        "    elif entity_type == \"company\":\n",
        "        entities = [{\"text\": company, \"type\": \"Organization\"}]\n",
        "    elif entity_type == \"weather\":\n",
        "        entities = [{\"text\": weather_condition, \"type\": \"Weather\"}, {\"text\": location, \"type\": \"Location\"}]\n",
        "    elif entity_type == \"car\":\n",
        "        entities = [{\"text\": car_model, \"type\": \"Car\"}, {\"text\": location, \"type\": \"Location\"}]\n",
        "    elif entity_type == \"event\":\n",
        "        entities = [{\"text\": event, \"type\": \"Event\"}, {\"text\": location, \"type\": \"Location\"}]\n",
        "    elif entity_type == \"product\":\n",
        "        entities = [{\"text\": product, \"type\": \"Product\"}, {\"text\": company, \"type\": \"Organization\"}]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": f\"Extract entities in JSON format from: {sentence}\",\n",
        "        \"response\": json.dumps({\"entities\": entities})\n",
        "    }\n",
        "\n",
        "# Generate dataset\n",
        "synthetic_data = [generate_sample() for _ in range(1200)]  # Increase for more variety\n",
        "\n",
        "# Preview sample\n",
        "print(synthetic_data[:3])\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_list(synthetic_data)\n",
        "print(dataset)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TvAzwZ25ET_F",
        "outputId": "41b45354-8d97-467a-cb9b-d59c417e17a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n\\nimport random\\nimport json\\nfrom datasets import Dataset\\n\\n# Sample data to randomize\\nnames = [\"Alice Johnson\", \"Bob Smith\", \"Charlie Davis\", \"Diana Green\", \"Ethan Brown\", \"Fiona White\", \"George Young\", \"Hannah Black\", \"Isaac Moore\", \"Julia Scott\"]\\ncompanies = [\"Google\", \"OpenAI\", \"Microsoft\", \"Apple\", \"Facebook\", \"Amazon\", \"Netflix\", \"Tesla\", \"Nvidia\", \"Adobe\"]\\nprofessions = [\"engineer\", \"designer\", \"manager\", \"researcher\", \"scientist\", \"consultant\", \"analyst\", \"developer\", \"specialist\", \"director\"]\\nlocations = [\"New York\", \"San Francisco\", \"London\", \"Berlin\", \"Toronto\", \"Tokyo\", \"Paris\", \"Sydney\", \"Singapore\", \"Dubai\"]\\n\\ndef generate_sample():\\n    name = random.choice(names)\\n    company = random.choice(companies)\\n    profession = random.choice(professions)\\n    location = random.choice(locations)\\n\\n    templates = [\\n        f\"{name} works at {company} as a {profession}.\",\\n        f\"{name} is a {profession} at {company} based in {location}.\",\\n        f\"{company} employs {name} as a {profession}.\",\\n        f\"In {location}, {name} works as a {profession} at {company}.\",\\n        f\"{name}, a {profession}, recently joined {company} in {location}.\"\\n    ]\\n\\n    sentence = random.choice(templates)\\n\\n    # Build response JSON\\n    entities = [\\n        {\"text\": name, \"type\": \"Person\"},\\n        {\"text\": company, \"type\": \"Organization\"},\\n        {\"text\": location, \"type\": \"Location\"} if \"in \" + location in sentence or \"based in \" + location in sentence else None\\n    ]\\n    entities = [e for e in entities if e is not None]\\n\\n    return {\\n        \"prompt\": f\"Extract entities in JSON format from: {sentence}\",\\n        \"response\": json.dumps({\"entities\": entities})\\n    }\\n\\n# Generate 400+ samples\\nsynthetic_data = [generate_sample() for _ in range(420)]\\nprint(synthetic_data)\\n# Convert to HuggingFace Dataset\\ndataset = Dataset.from_list(synthetic_data)\\ndataset\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "import random\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Sample data to randomize\n",
        "names = [\"Alice Johnson\", \"Bob Smith\", \"Charlie Davis\", \"Diana Green\", \"Ethan Brown\", \"Fiona White\", \"George Young\", \"Hannah Black\", \"Isaac Moore\", \"Julia Scott\"]\n",
        "companies = [\"Google\", \"OpenAI\", \"Microsoft\", \"Apple\", \"Facebook\", \"Amazon\", \"Netflix\", \"Tesla\", \"Nvidia\", \"Adobe\"]\n",
        "professions = [\"engineer\", \"designer\", \"manager\", \"researcher\", \"scientist\", \"consultant\", \"analyst\", \"developer\", \"specialist\", \"director\"]\n",
        "locations = [\"New York\", \"San Francisco\", \"London\", \"Berlin\", \"Toronto\", \"Tokyo\", \"Paris\", \"Sydney\", \"Singapore\", \"Dubai\"]\n",
        "\n",
        "def generate_sample():\n",
        "    name = random.choice(names)\n",
        "    company = random.choice(companies)\n",
        "    profession = random.choice(professions)\n",
        "    location = random.choice(locations)\n",
        "\n",
        "    templates = [\n",
        "        f\"{name} works at {company} as a {profession}.\",\n",
        "        f\"{name} is a {profession} at {company} based in {location}.\",\n",
        "        f\"{company} employs {name} as a {profession}.\",\n",
        "        f\"In {location}, {name} works as a {profession} at {company}.\",\n",
        "        f\"{name}, a {profession}, recently joined {company} in {location}.\"\n",
        "    ]\n",
        "\n",
        "    sentence = random.choice(templates)\n",
        "\n",
        "    # Build response JSON\n",
        "    entities = [\n",
        "        {\"text\": name, \"type\": \"Person\"},\n",
        "        {\"text\": company, \"type\": \"Organization\"},\n",
        "        {\"text\": location, \"type\": \"Location\"} if \"in \" + location in sentence or \"based in \" + location in sentence else None\n",
        "    ]\n",
        "    entities = [e for e in entities if e is not None]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": f\"Extract entities in JSON format from: {sentence}\",\n",
        "        \"response\": json.dumps({\"entities\": entities})\n",
        "    }\n",
        "\n",
        "# Generate 400+ samples\n",
        "synthetic_data = [generate_sample() for _ in range(420)]\n",
        "print(synthetic_data)\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_list(synthetic_data)\n",
        "dataset\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "0921b68349194140b6ad3ecdd7e81d47",
            "66ce782ee35241f2838cf8e47d70a139",
            "0aab22179c1f41b0b71418162c96117c",
            "401c5e38cfff42859e27abdd929940cc",
            "484ba4b3ce1a42bc8adcfc1331f5c2d4",
            "a6a4b6882948438ca3d79671f4ad2fd8"
          ]
        },
        "id": "GymihlMOM6mD",
        "outputId": "31634c1e-b2e1-4115-98a4-7d39b238a34f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0921b68349194140b6ad3ecdd7e81d47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66ce782ee35241f2838cf8e47d70a139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0aab22179c1f41b0b71418162c96117c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "401c5e38cfff42859e27abdd929940cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "484ba4b3ce1a42bc8adcfc1331f5c2d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6a4b6882948438ca3d79671f4ad2fd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/420 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"tiiuae/falcon-rw-1b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Falcon doesn't have a pad token\n",
        "\n",
        "def preprocess(example):\n",
        "    full_prompt = f\"{example['prompt']}\\n{example['response']}\"\n",
        "    tokenized = tokenizer(full_prompt, truncation=True, padding=\"max_length\", max_length=256)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZp5geDGnJi"
      },
      "source": [
        "## Loading the model raw without fine tuning to check results before finetuning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "3867fdc2ca3c4eaebbaa7f9d41995b06",
            "dbb3a7cf43d240feafa70ccb9d17e23f",
            "ede95e4e1661488f9e51303bcb01dad3",
            "3a33961920ed41948efd53ee194842c8"
          ]
        },
        "id": "eq34WQB8FMQn",
        "outputId": "b2e5b672-a2f8-4e94-a339-a7dd1d9e947f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3867fdc2ca3c4eaebbaa7f9d41995b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbb3a7cf43d240feafa70ccb9d17e23f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ede95e4e1661488f9e51303bcb01dad3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a33961920ed41948efd53ee194842c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Embedding(50257, 2048)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGO5Vf0qGvQr"
      },
      "source": [
        "## Generate response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7kU4HvhmGqur",
        "outputId": "27d3bcc2-cda9-4afa-99c6-ebb9ed94f4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract entities in JSON format from: Elon Musk founded Tesla and SpaceX. He is also the CEO of SolarCity, a solar energy company. He is also the founder of OpenAI, a company that develops artificial intelligence.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in California.\n",
            "The company is based in\n"
          ]
        }
      ],
      "source": [
        "def extract_entities_raw(text):\n",
        "    prompt = f\"Extract entities in JSON format from: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(extract_entities_raw(\"Elon Musk founded Tesla and SpaceX.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUiTUtbYG2sd"
      },
      "source": [
        "## ⚙️ Set Training Configuration\n",
        "\n",
        "We use PEFT (LoRA) + TRL's SFTTrainer to train efficiently even on a T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5VF40DmdGzjQ",
        "outputId": "e7120a3e-aec6-493a-90f8-3c0088f84340"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,572,864 || all params: 1,313,101,824 || trainable%: 0.1198\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "base_model.enable_input_require_grads()\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab1BFxlPYzxN"
      },
      "source": [
        "## train the model and save finetuned weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9UXB27G6bQ",
        "outputId": "38530908-202a-41dd-b551-5c0b89356e6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-8978f8251423>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"falcon-entity-extractor-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"falcon-entity-extractor-lora\")\n",
        "tokenizer.save_pretrained(\"falcon-entity-extractor-lora\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMi9PxIMY-jf"
      },
      "source": [
        "## load weights for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WKJLOi3ONnH"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "model_ft = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model_ft.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = PeftModel.from_pretrained(model_ft, \"falcon-entity-extractor-lora\")\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_vFAXFwRbOA"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text, max_new_tokens=100):\n",
        "    prompt = f\"Extract entities in JSON format from: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"\\n\" in decoded:\n",
        "        return decoded.split(\"\\n\", 1)[-1].strip()\n",
        "    return decoded.strip()\n",
        "\n",
        "# ✅ Test After Fine-Tuning\n",
        "print(extract_entities(\"what tablets should i take for my headacs\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V646kiXRw5U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_9zo2XzTE2j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJ0q3QiTG67"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "name": "fine_tune-medical-data.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}